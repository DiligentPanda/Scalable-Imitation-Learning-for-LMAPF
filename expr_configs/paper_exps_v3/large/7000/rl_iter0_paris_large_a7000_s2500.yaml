expr_group: LMAPF
expr_name: paper_exps_v3_rl_iter0_paris_large_a10000_s2500_basic_cnn_re_embed_perm5_bn_1block
log_dir: ./logs             #log directory
seed: 0
eval_only: False            #True if only want evaluation

distributed:
  use: False
  auto_connect:
  auto_copy:
  nodes:
    master:
      ip: "auto"
    workers:      
      - ip:

framework:
  name: "simple"                #framework name
  # max_rounds: 1         # psro rounds
  # meta_solver: "nash"         #'nash', 'uniform', 'pfsp'
  sync_training: True
  stopper:
    type: "max_step_stopper"
    kwargs:
      max_steps: 5000          # max iterations: we should change the name!
agent_manager:
  num_agents: 1 
  share_policies: False # whether global agents (teams) share the same policy. Not whether agents in a team share the same policy, which should be defined in the actor.

evaluation_manager:
  num_eval_rollouts: 32

policy_data_manager:
  update_func: "LMAPF" 
  fields:
    throughput:
      type: "matrix"
      missing_value: 0
        
monitor:
  type: "local"   #'remote': wandb or 'local': tensorboard
  distributed:
    resources:
      num_cpus: 1
      resources:
        - ["node:${distributed.nodes.master.ip}",0.01]

rollout_manager:
  distributed:
    resources:
      num_cpus: 1
      num_gpus: 0.004
      resources:
        - ["node:${distributed.nodes.master.ip}",0.01]
  num_workers: 32     # numbers of parallel rollout worker, one env per worker by default
  seed: 12345
  saving_interval: 100 # the frequency of dumping model's weight
  batch_size: 32
  eval_batch_size: 32
  eval_freq: 10 # epochs
  rollout_metric_cfgs:
    reward:
      type: "sliding"
      window_size: 10
      init_list: [-10000]
    throughput:
      type: "sliding"
      window_size: 10
      init_list: [0,0,0]
  worker:
    distributed:
      resources:
        num_cpus: 1
        num_gpus: 0.004
    rollout_length: 50                                    #episode length
    eval_rollout_length: 2500
    sample_length: 0                                     #every $sample_length traj will be push to buffer during rollout, sample length 0 means push when done
    padding_length: # of not use in gr_football
    rollout_func:
      module: "rollout_func_LMAPF"
      func: "rollout_func"
    eval_rollout_func:
      module: "rollout_func_LMAPF"
      func: "rollout_func" 
    episode_mode: "traj"
    envs:
      - cls: "LMAPF"
        id_prefix: "LMAPF"
        instances: 
        learn_to_follow_maps_path: 
        agent_bins:
        map_path: "lmapf_lib/data/paper_exp_v3/large/Paris_1_256.map"
        map_weights_path: "lmapf_lib/weight/paris_large_w001.w" 
        num_robots: 7000
        device: "cuda"
        rollout_length: 2500
        gae_gamma: 1.0
        gae_lambda: 0.95
        mappo_reward: False
        # reward_config:
        #   goal_reward: 0
        #   official_reward: 1
        WPPL:
          mode: "PIBT-RL" #"none" #"PIBT-LNS" #"PIBT-RL-LNS"
          window_size: 15
          num_threads: 1
          max_iterations: 5000
          time_limit: 0.0 # useless when set max_iterations
          verbose: false
    credit_reassign:

training_manager:
  distributed:
    resources:
      num_cpus: 1
      resources:
        - ["node:${distributed.nodes.master.ip}",0.01]
 
  master_addr: "127.0.0.1"
  master_port:  "12774"
  local_queue_size: 1
  batch_size: 32 # how many data sample from DatasetServer per time.
  num_prefetchers: 1
  data_prefetcher:
    distributed:
      resources:
        num_cpus: 1
  num_trainers: 4 # equals to number of GPUs by default
  # control the frequency of remote parameter update
  update_interval: 1
  gpu_preload: False
  trainer:
    distributed:
      resources:
        num_cpus: 1
        num_gpus: 0.6
        resources:
          - ["node:${distributed.nodes.master.ip}",0.01]
    optimizer: "Adam"
    actor_lr: 5.e-4
    critic_lr: 5.e-4
    backbone_lr: 5.e-4
    opti_eps: 1.e-5
    weight_decay: 0.0
    lr_decay: False            #update_linear_schedule
    lr_decay_epoch: 2000      #how many rollout steps till zero

data_server:
  distributed:
    resources:
      num_cpus: 1
      resources:
        - ["node:${distributed.nodes.master.ip}",0.01]
  table_cfg:
    capacity: 1000
    sampler_type: "lumrf"
    sample_max_usage: 1
    rate_limiter_cfg:
      min_size: ${training_manager.batch_size}
      # r_w_ratio: 10.0
  read_timeout: 1
  guiding_policy: 
  imitation_dataset:

policy_server:
  distributed:
    resources:
      num_cpus: 1
      resources:
        - ["node:${distributed.nodes.master.ip}",0.01]

populations:
  - population_id: default # population_id
    algorithm:
      name: "MAPPO"
      model_config:
        model: "LMAPF.basic_cnn_re_embed_perm5_bn"       # model type
        initialization:
          use_orthogonal: True
          gain: 1.
        backbone:
        actor:
          num_re_embed_blocks: 1
          hidden_dim: 32
        critic:
          num_re_embed_blocks: 1
          hidden_dim: 32


      # set hyper parameter
      custom_config:
        # FE_cfg:
        #   num_robots: 5    #config sent to the Feature Encoder
        sub_algorithm_name: MAPPO
        # num_agents: 3 
        other_clip_param: 0.125

        # gamma: 0.99
        use_cuda: False  # enable cuda or not
        use_q_head: False
        ppo_epoch: 15
        num_mini_batch: 6  # the number of mini-batches
        max_grad_norm: 10.0
        
        # return_mode: new_gae
        # gae:
        #   gae_lambda: 0.95
        # vtrace:
        #   clip_rho_threshold: 1.0
        #   clip_pg_rho_threshold: 100.0

        use_rnn: False
        # this is not used, instead it is fixed to last hidden in actor/critic
        rnn_layer_num: 1
        rnn_data_chunk_length: 16

        use_feature_normalization: True
        use_popart: True
        popart_beta: 0.99999

        entropy_coef: 0.0
        clip_param: 0.2
        # kl_early_stop: 0.01
        value_loss_coef: 1.0
        guiding_loss_coef: 0.0
        policy_loss_coef: 1.0

      policy_init_cfg:
        agent_0: # agent_id
          new_policy_ctr_start: -1
          init_cfg:
            - condition: "==0" # condition if True in order
              strategy: random #pretrained
              policy_id: #pretrained
              policy_dir: #logs/LMAPF/basic_cnn_re_embed_perm5_bn_1block_BC_iter1_random_32_32_400agents_1000steps_debug/2024-06-02-09-41-26/agent_0/agent_0-default-1/best
          initial_policies: